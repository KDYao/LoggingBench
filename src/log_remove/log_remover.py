"""
This script removes logging statements form java projects
"""
import shutil
import tarfile

import src.util.utils as ut
import os
import re
import itertools
import json
import pandas as pd
import ast
import subprocess
from collections import defaultdict

logger = ut.setlogger(
    f_log='../../log/log_removal/log_removal.log',
    logger="log_remover",
)
lock = ut.setRWLock()

class LogRemover:
    def __init__(self, f_removal,
                 sample_dir='../../result/proj_sample',
                 f_log_stats='../../conf/log_all_stats.csv',
                 repeats=1):
        self.sample_dir = sample_dir
        if not os.path.isdir(sample_dir):
            os.makedirs(sample_dir)
        self.f_removal = f_removal
        ut.create_folder_if_not_exist(os.path.dirname(f_removal))
        # f_removal records processed files and lines in JSON
        if os.path.isfile(f_removal):
            with open(f_removal) as r:
                self.logging_remove_json = json.load(r)
        else:
            self.logging_remove_json = defaultdict(dict)
        self.d_proj_size = '../../result/proj_size'
        self.sample_sizes = ['small', 'medium', 'large', 'vlarge']
        self.repeats = repeats
        self.lu_levels = self.load_lu_levels()
        self.df_proj_lus = self.load_lu_per_project(f_log_stats)
        self.d_clean_project_root = ut.getPath('CLEANED_PROJ_ROOT', ischeck=False)
        ut.create_folder_if_not_exist(self.d_clean_project_root)
        # Generate sampled projects from each size
        self.project_sample()

    def load_lu_per_project(self, f):
        """
        Load log_all_stats.csv and get the LUs used in each project
        This file currently is not being tracked since its generated by Chen not us
        We do not wish to disclose too much details before we have Chen's permission
        Parameters
        ----------
        f

        Returns
        -------

        """
        df = ut.csv_loader(f)
        df['project_id'] = df['project'].apply(lambda x: int(x.split('-')[0]))
        keep_cols = ['project_id', 'others'] + [x for x in self.lu_levels.keys() if x in df.columns]
        return df[keep_cols]


    def load_lu_levels(self, f='../../conf/lu_levels.json'):
        """
        Load logging utilities
        Parameters
        ----------
        f
        extra

        Returns
        -------

        """
        with open(f) as r:
         lu_levels = json.load(r)
        return lu_levels

    def filter_row(self, row):
        """
        Check if a row contain general logging utilities
        If listed LU is not in recorded dataset as a separate column, will check it through the 'others' column
        Parameters
        ----------
        row
        cols

        Returns
        If is general LU, and the LU used
        -------

        """
        general_lus = []
        for x in self.lu_levels.keys():
            if x in row.keys():
                ig = row[x]
            else:
                if isinstance(row['others'], str):
                    ig = (x in row['others'])
                else:
                    continue
            if ig is True:
                general_lus.append(x)
        return len(general_lus) > 0, general_lus


    def filter_projects_by_lus(self, df):
        """
        Filter projects by selected logging utilities
        Returns
        -------
        """
        df = pd.merge(df, self.df_proj_lus, on='project_id')
        df[['is_general', 'general_lus']] = df.apply(func=self.filter_row, axis=1, result_type='expand')
        return df[df['is_general'] is True]



    def project_sample(self, sample_percentage=0.1, overwrite=False):
        """
        Sample 10% of the projects from each size
        Parameters
        ----------
        sample_percentage: The percentage of sampling
        overwrite: if overwrite existing files

        Returns
        -------

        """
        sloc_dir = '../../result/proj_sloc'
        ut.print_msg_box('Sample {}% projects from each size'.format(sample_percentage * 100))
        # Concatenate all size types to be analyzed
        for repeat in range(1, self.repeats + 1):
            for size_type in self.sample_sizes:
                f_projects_sample = os.path.join(self.sample_dir, 'sample_{}_sloc_{}.csv'.format(repeat, size_type))
                if os.path.isfile(f_projects_sample):
                    if overwrite:
                        print('Overwrite existing project {}'.format(os.path.basename(f_projects_sample)))
                    else:
                        print('Sample projects already exist in {}; skip'.format(os.path.basename(f_projects_sample)))
                        continue
                df_projects = ut.csv_loader(os.path.join(sloc_dir, 'filesize_sloc_{}.csv'.format(size_type)))
                df_projects = self.filter_projects_by_lus(df=df_projects)
                df_projects_sample = df_projects.sample(frac=sample_percentage, random_state=repeat)
                df_projects_sample.to_csv(f_projects_sample, index=False)

    def get_total_project_size(self, proj_id_list):
        """
        Calculate the total size of selected projects after decompression
        Returns
        -------
        """
        # Merge all projects from size calculation
        df_merged = pd.concat(
            [ut.csv_loader(
                os.path.join(self.d_proj_size, 'filesize_mb_{}.csv'.format(size_type))
            ) for size_type in self.sample_sizes])
        df_merged = df_merged.loc[df_merged['project_id'].isin(proj_id_list)]
        return ut.convert_size(df_merged['size_mb'].sum() * 1024 * 1024)


    def logger_detector(self, repeat_idx):
        """
        Detect java files with logging statements such as logger, etc. followed by a function call.
        Returns
        -------
        """
        # Merge all sampled projects under the same repeat index
        df_merged = pd.concat(
            [ut.csv_loader(
                os.path.join(self.sample_dir, 'sample_{}_sloc_{}.csv'.format(repeat_idx, size_type))
            ) for size_type in self.sample_sizes])

        total_projects_count = df_merged['project_id'].count()
        total_projects_size = self.get_total_project_size(list(df_merged['project_id']))
        total_num_java = df_merged['Count'].sum()
        total_uncompressed_java_size = ut.convert_size(df_merged['Bytes'].sum())
        # TODO: Also add the size of total sizes
        ut.print_msg_box('Projects Summary\n'
                         'Repeat ID:{rep_id}\n'
                         'Total Projects:{proj_count}\n'
                         'Total Projects Size:{proj_size}\n'
                         'Total Number of Java Files:{num_f_java}\n'
                         'Total Sizes of Java Files:{java_size}'.format(rep_id=repeat_idx,
                                                                        proj_count=total_projects_count,
                                                                        proj_size=total_projects_size,
                                                                        num_f_java=total_num_java,
                                                                        java_size=total_uncompressed_java_size))

        # Preserve for parallelism
        for df in ut.chunkify(df_merged, ut.getWorkers()):
            log_remove_lst = [self.remove_logging(row=row, repeat_idx=repeat_idx) for idx, row in df.iterrows()]
            for lrm in log_remove_lst:
                if lrm is not None:
                    log_remove_repo_id, log_remove_repo_detail = lrm
                    self.logging_remove_json[log_remove_repo_id] = log_remove_repo_detail
            # Save Json
            with lock:
                if os.path.isfile(self.f_removal):
                    with open(self.f_removal, 'r+') as f_update:
                        f_update.seek(0)
                        f_update.write(json.dumps(self.logging_remove_json, indent=4))
                        f_update.truncate()
                else:
                    with open(self.f_removal, 'w') as f_update:
                        f_update.write(json.dumps(self.logging_remove_json, indent=4))



    def remove_logging(self, row, repeat_idx):
        """
        Decompress selected java projects and remove logging statements from them
        Parameters
        ----------
        row: dataframe row, records the information of a project
        repeat_idx: The repeat index of current experiment

        Returns
        -------

        """
        repo_path = row['repo_path']
        repo_id = int(row['project_id'])
        owner_repo = row['owner_repo']

        # Temp location to store project
        tmp_out_dir = os.path.abspath(os.path.join(
            *[self.d_clean_project_root, 'repeat_%d' % repeat_idx, str(repo_id)]
        ))

        # Skip remove logging if this project has already been log removed
        if str(repo_id) in self.logging_remove_json.keys() and os.path.isdir(tmp_out_dir):
            print('Project %s has already been log removed; skip' % owner_repo)
            return

        # FIXME:###### Only for local testing
        ############################
        repo_path = os.path.join(ut.getPath('REPO_ZIPPED_ROOT'), os.path.basename(repo_path))
        if not os.path.isfile(repo_path): return
        ############################

        if not os.path.isfile(repo_path):
            logger.error('Cannot find project %s at %s' % (owner_repo, repo_path))
            return
        print('Start decompression and logging removal from %s' % owner_repo)
        # Decompress
        self.decompress_project(f_tar=repo_path, out_d=tmp_out_dir)

        general_lus = ast.literal_eval(row['general_lus'])
        function_names = set(itertools.chain.from_iterable([self.lu_levels[lu] for lu in general_lus]))
        cmd = 'grep -rinE "(.*log.*)\.({funcs})\(.*\)" --include=\*.java .'.format(
            funcs='|'.join(function_names))
        out_raw = subprocess.check_output(cmd, shell=True, cwd=tmp_out_dir)
        try:
            out = out_raw.decode('utf-8')
        except UnicodeError:
            out = out_raw.decode('iso-8859-1')
        # Process results
        re_match = re.compile(r'^./(.*\.java)\:(\d+)\:(.*)$')
        proj_logging_removal = defaultdict(lambda: defaultdict(str))
        for line in out.split('\n'):
            if line == "": continue
            f_path, line_num, line_content = re_match.match(line).groups()
            proj_logging_removal[f_path][int(line_num)] = line_content

        # Iterate each file and remove logging statements
        for f_path, line_info in proj_logging_removal.items():
            f = os.path.join(tmp_out_dir, f_path)
            # Remove logging statements by line number
            # Cannot write to original file directly since > has a higher priority
            cmd = "awk '%s {gsub(/.*/,\"\")}; {print}' %s > %s_lrm_temp && mv %s_lrm_temp %s" % \
                  (' || '.join(['NR == %d' % x for x in line_info.keys()]), f, f, f, f)
            p = subprocess.Popen(cmd, shell=True)

            try:
                p.communicate()
            except Exception as ex:
                logger.error('Fail to remove log for {f}; {ex}'.format(f, str(ex)))

        # Record result in json
        return (repo_id, proj_logging_removal)

    def decompress_project(self, f_tar, out_d):
        """
        Decompress project into a temporary location
        Parameters
        ----------
        f_tar
        repo_id

        Returns
        -------

        """
        # Clean temp project if it exists. This could happen when a previous job collapsed
        if os.path.isdir(out_d):
            shutil.rmtree(out_d)

        # Decompress tar to temp folder
        tar = tarfile.open(f_tar, "r:gz")
        tar.extractall(path=out_d)
        tar.close()


if __name__ == '__main__':
    f_removal = '../../result/log_remove/logging_removal_lines.json'
    logremover = LogRemover(f_removal)
    for repeat_idx in range(1, 1 + logremover.repeats):
        logremover.logger_detector(repeat_idx)